\documentclass[11pt, a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{multirow}
\usepackage{titlesec}

% Optimized margins for compact layout
\geometry{margin=0.8in, top=0.9in, bottom=0.9in}

% Reduce spacing around sections
\titlespacing*{\section}{0pt}{10pt}{6pt}
\titlespacing*{\subsection}{0pt}{8pt}{4pt}
\titlespacing*{\subsubsection}{0pt}{6pt}{3pt}

% Compact paragraph spacing
\setlength{\parskip}{4pt}
\setlength{\itemsep}{2pt}

\title{Sports vs. Politics: A Text Classification Study}
\author{Prasangeet Dongre \\ Roll No: B23CH1033}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
This report details the design and implementation of a text classification system capable of distinguishing between sports-related and politics-related documents. Using a dataset collected from RSS feeds, we explore the efficacy of Bag-of-Words (BoW) and Term Frequency-Inverse Document Frequency (TF-IDF) feature representations across three machine learning algorithms: Naive Bayes, Logistic Regression, and Support Vector Machines (SVM). Our experiments demonstrate that linear models like Logistic Regression and Support Vector Machines (SVM) combined with Bag-of-Words (BoW) features achieve peak performance of 98.67\% accuracy, highlighting the distinct lexical properties of the two categories even as the dataset size increases.
\end{abstract}

\tableofcontents
\vspace{10pt}

\section{Introduction}
Text classification is a fundamental task in Natural Language Processing (NLP), with applications ranging from spam detection to sentiment analysis. The objective of this project is to build a binary classifier that categorizes text documents into one of two distinct classes: \textit{Sports} (specifically football/cricket etc.) or \textit{Politics} (elections, policy, government).

Understanding the differences in vocabulary and structure between these domains allows us to effectively deploy machine learning models. This report outlines the data collection process, preprocessing steps, feature extraction techniques, and a comprehensive evaluation of classification performance.

\section{Data Collection}
To ensure a realistic evaluation, we constructed a dataset from live news sources rather than using pre-packaged corpora. Data was collected using the \textbf{RSS (Really Simple Syndication)} protocol, which provides structured XML feeds of updated content.

\subsection{Sources}
We targeted high-traffic news outlets known for their distinct categorization. The following RSS feeds were used:
\begin{itemize}
    \item \textbf{Sports}: ESPN News, BBC Sport, Sky Sports Football, The Guardian Sport, and Yahoo Sports.
    \item \textbf{Politics}: BBC News Politics, NYT Politics, The Guardian Politics, Politico, and Reuters Politics.
\end{itemize}

\subsection{Extraction Process}
A Python script utilizing the \texttt{feedparser} and \texttt{BeautifulSoup} libraries was developed. For each entry in the RSS feed:
\begin{enumerate}
    \item The article link was extracted.
    \item An HTTP request fetched the full HTML content.
    \item \texttt{BeautifulSoup} parsed the HTML to extract text within \texttt{<p>} tags.
    \item Articles shorter than 800 characters were discarded to ensure sufficient content for classification.
\end{enumerate}

\section{Dataset Description and Analysis}
The final dataset consists of \textbf{372 documents}. This larger collection provides a more robust foundation for training and evaluating text classification models.

\subsection{Class Distribution}
The dataset is imbalanced, with a bias towards sports articles:
\begin{itemize}
    \item \textbf{Sports}: 213 documents (approx. 57\%)
    \item \textbf{Politics}: 159 documents (approx. 43\%)
\end{itemize}
To mitigate this during training, we employed stratified sampling for our train-test splits to ensure both sets reflected this distribution.

\subsection{Statistical Analysis}
\begin{itemize}
    \item \textbf{Average Document Length}: 448 words.
    \item \textbf{Vocabulary Size (BoW)}: 16,635 unique tokens.
    \item \textbf{Vocabulary Size (TF-IDF, including bigrams)}: 50,000 features.
\end{itemize}
The high vocabulary size relative to the number of documents suggests a sparse feature space, which linear models like SVM and Naive Bayes handle well.

\section{Methodology}

\subsection{Text Preprocessing}
Before feature extraction, raw text underwent normalization:
\begin{itemize}
    \item \textbf{Lowercasing}: Converting all text to lowercase to treat "Goal" and "goal" identically.
    \item \textbf{Regex Cleaning}: Removing URLs, special characters, and non-alphabetic tokens.
    \item \textbf{Stopword Removal}: Using NLTK's English stopword list to remove common words (e.g., "the", "and") that carry little semantic meaning.
    \item \textbf{Tokenization}: Splitting text into individual words.
\end{itemize}

\subsection{Feature Representation}
We explored two methods to convert text into numerical vectors:

\subsubsection{Bag of Words (BoW) - Unigrams}
BoW represents text as a vector of word counts. It ignores grammar and word order but captures keyword frequency using single words (unigrams).
\begin{itemize}
    \item \textbf{Pros}: Simple, interpretable.
    \item \textbf{Cons}: Ignores context, weighs frequent common words heavily (if not filtered).
    \item \textbf{Vocabulary}: 13,236 unique unigrams.
\end{itemize}

\subsubsection{TF-IDF with N-grams (1-2 grams)}
TF-IDF weighs words by how unique they are to a specific document across the corpus. Critically, we included \textbf{n-grams (bigrams)} to capture two-word phrases which are often more discriminative than single words.
\begin{itemize}
    \item \textbf{N-gram Range}: Both unigrams (1-gram) and bigrams (2-grams)
    \item \textbf{Example Bigrams}: "prime minister", "champions league", "election campaign", "goal scored"
    \item \textbf{Pros}: Penalizes common words, highlights distinctive terms, captures multi-word expressions.
    \item \textbf{Cons}: High dimensionality (50,000 features including bigrams).
    \item \textbf{Implementation}: \texttt{TfidfVectorizer(ngram\_range=(1, 2))}
\end{itemize}

\subsection{Machine Learning Models}
We trained three classifiers:
\begin{enumerate}
    \item \textbf{Multinomial Naive Bayes}: A probabilistic classifier based on Bayes' theorem. It assumes independence between features. Excellent baseline for text.
    \item \textbf{Logistic Regression}: A linear model that estimates probabilities using a logistic function.
    \item \textbf{Support Vector Machine (SVM)}: Finds the hyperplane that best separates the classes with the maximum margin.
\end{enumerate}

\section{Quantitative Comparisions and Results}
The dataset was split into training (80\%) and testing (20\%) sets.

\subsection{Accuracy Comparison}
\begin{table}[H]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{BoW (Unigrams)} & \textbf{TF-IDF (1-2 grams)} \\
\midrule
Naive Bayes & 0.97 & 0.96 \\
Logistic Regression & \textbf{0.99} & 0.97 \\
SVM & \textbf{0.99} & 0.97 \\
\bottomrule
\end{tabular}
\caption{Model Accuracy: Comparing Unigram vs N-gram Features}
\label{tab:results}
\end{table}

\subsection{Detailed Performance Metrics}
Beyond raw accuracy, we examine precision, recall, and F1-score for each class to understand model behavior comprehensively.

\subsubsection{Naive Bayes Performance}
\begin{table}[H]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & \multicolumn{3}{c}{\textbf{Bag of Words}} & \multicolumn{3}{c}{\textbf{TF-IDF}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & Precision & Recall & F1 & Precision & Recall & F1 \\
\midrule
Politics & 1.00 & 0.94 & 0.97 & 1.00 & 0.91 & 0.95 \\
Sports & 0.96 & 1.00 & 0.98 & 0.93 & 1.00 & 0.97 \\
\midrule
\textbf{Accuracy} & \multicolumn{3}{c}{\textbf{0.97}} & \multicolumn{3}{c}{\textbf{0.96}} \\
\bottomrule
\end{tabular}
\caption{Naive Bayes Classification Metrics}
\label{tab:nb_metrics}
\end{table}

\subsubsection{Logistic Regression Performance}
\begin{table}[H]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & \multicolumn{3}{c}{\textbf{Bag of Words}} & \multicolumn{3}{c}{\textbf{TF-IDF}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & Precision & Recall & F1 & Precision & Recall & F1 \\
\midrule
Politics & 1.00 & 0.97 & 0.98 & 1.00 & 0.94 & 0.97 \\
Sports & 0.98 & 1.00 & 0.99 & 0.96 & 1.00 & 0.98 \\
\midrule
\textbf{Accuracy} & \multicolumn{3}{c}{\textbf{0.99}} & \multicolumn{3}{c}{\textbf{0.97}} \\
\bottomrule
\end{tabular}
\caption{Logistic Regression Classification Metrics}
\label{tab:logreg_metrics}
\end{table}

\subsubsection{Support Vector Machine Performance}
\begin{table}[H]
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Class} & \multicolumn{3}{c}{\textbf{Bag of Words}} & \multicolumn{3}{c}{\textbf{TF-IDF}} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
 & Precision & Recall & F1 & Precision & Recall & F1 \\
\midrule
Politics & 1.00 & 0.97 & 0.98 & 1.00 & 0.94 & 0.97 \\
Sports & 0.98 & 1.00 & 0.99 & 0.96 & 1.00 & 0.98 \\
\midrule
\textbf{Accuracy} & \multicolumn{3}{c}{\textbf{0.99}} & \multicolumn{3}{c}{\textbf{0.97}} \\
\bottomrule
\end{tabular}
\caption{SVM Classification Metrics}
\label{tab:svm_metrics}
\end{table}

\subsection{Confusion Matrices}
Confusion matrices provide a detailed breakdown of correct and incorrect predictions for each class.

\subsubsection{Best Models: Logistic Regression \& SVM (BoW)}
\begin{table}[H]
\centering
\begin{tabular}{cc|cc}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Predicted}} \\
\multicolumn{2}{c|}{} & Politics & Sports \\
\cline{2-4}
\multirow{2}{*}{\textbf{Actual}} & Politics & 31 & 1 \\
 & Sports & 0 & 43 \\
\end{tabular}
\caption{Confusion Matrix: LogReg/SVM + BoW (98.67\% Accuracy)}
\label{tab:cm_best_bow}
\end{table}

\subsubsection{Naive Bayes + BoW}
\begin{table}[H]
\centering
\begin{tabular}{cc|cc}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Predicted}} \\
\multicolumn{2}{c|}{} & Politics & Sports \\
\cline{2-4}
\multirow{2}{*}{\textbf{Actual}} & Politics & 30 & 2 \\
 & Sports & 0 & 43 \\
\end{tabular}
\caption{Confusion Matrix: Naive Bayes + BoW (97.33\% Accuracy)}
\label{tab:cm_nb_bow}
\end{table}

\subsubsection{SVM + TF-IDF (Best TF-IDF Model)}
\begin{table}[H]
\centering
\begin{tabular}{cc|cc}
\multicolumn{2}{c}{} & \multicolumn{2}{c}{\textbf{Predicted}} \\
\multicolumn{2}{c|}{} & Politics & Sports \\
\cline{2-4}
\multirow{2}{*}{\textbf{Actual}} & Politics & 30 & 2 \\
 & Sports & 0 & 43 \\
\end{tabular}
\caption{Confusion Matrix: SVM + TF-IDF (Consistent Performance)}
\label{tab:cm_svm_tfidf}
\end{table}

\subsection{Analysis of Results}
\begin{itemize}
    \item \textbf{High Baseline Performance}: All models achieved over 96\% accuracy. This indicates that even with a larger and more diverse dataset of 372 documents, the vocabulary remains highly discriminative.
    
    \item \textbf{Bag-of-Words Superiority}: Logistic Regression and SVM with BoW both achieved 98.67\% accuracy. They misclassified only one politics article as sports, while correctly identifying all 43 sports articles in the test set.
    
    \item \textbf{Improved TF-IDF Recall}: Unlike the initial small-scale experiment, the larger dataset allowed TF-IDF models to achieve high recall (over 91\%) for both classes. This suggests that the previous poor performance was indeed a result of insufficient data for the high-dimensional TF-IDF space.
    
    \item \textbf{SVM and LogReg Robustness}: Both linear models performed identically on the BoW features, demonstrating their effectiveness for text categorization in this domain.
\end{itemize}

\section{Limitations}
Despite the high accuracy, the system has limitations:
\begin{itemize}
    \item \textbf{Dataset Size}: While expanded to 372 documents, the dataset remains relatively specialized. High accuracy may partly stem from the specific RSS feeds selected.
    \item \textbf{Class Imbalance}: Although improved, the dataset still has a 57/43 split between sports and politics.
    \item \textbf{Context Sensitivity}: BoW and TF-IDF ignore word order (mostly). "The team defeated the rival" and "The rival defeated the team" have similar vector representations but opposite meanings.
\end{itemize}

\section{Conclusion}
We successfully built a text classifier for Sports vs. Politics. The experiments showed that for distinct topics and small datasets, simple Bag-of-Words representations often outperform more complex TF-IDF schemes.

\end{document}
